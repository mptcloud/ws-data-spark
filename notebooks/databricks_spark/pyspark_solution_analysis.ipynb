{"cells":[{"cell_type":"markdown","source":["## Overview\n\nThis notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. [DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html) is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.\n\nThis notebook is written in **Python** so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` syntax. Python, Scala, SQL, and R are all supported."],"metadata":{}},{"cell_type":"code","source":["# File location and type\nfile_location = \"/FileStore/tables/POIList.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\npoi_df = spark.read.format(file_type) \\\n  .option(\"header\",first_row_is_header) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location) \\\n  .toDF('POIID', 'Latitude', 'Longitude')\n\npoi_df.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- POIID: string (nullable = true)\n-- Latitude: double (nullable = true)\n-- Longitude: double (nullable = true)\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["poi_df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+---------+-----------+\nPOIID| Latitude|  Longitude|\n+-----+---------+-----------+\n POI1|53.546167|-113.485734|\n POI2|53.546167|-113.485734|\n POI3|45.521629| -73.566024|\n POI4| 45.22483| -63.232729|\n+-----+---------+-----------+\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["# File location and type\nfile_location = \"/FileStore/tables/DataSample.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndata_df = spark.read.format(file_type) \\\n  .option(\"header\",first_row_is_header) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location) \\\n  .toDF('ID', 'TimeSt', 'Country', 'Province', 'City', 'Latitude', 'Longitude')\n\ndata_df.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- ID: integer (nullable = true)\n-- TimeSt: timestamp (nullable = true)\n-- Country: string (nullable = true)\n-- Province: string (nullable = true)\n-- City: string (nullable = true)\n-- Latitude: double (nullable = true)\n-- Longitude: double (nullable = true)\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["data_df.show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+--------------------+-------+--------+---------+--------+---------+\n     ID|              TimeSt|Country|Province|     City|Latitude|Longitude|\n+-------+--------------------+-------+--------+---------+--------+---------+\n4516516|2017-06-21 00:00:...|     CA|      ON| Waterloo|43.49347|-80.49123|\n4516547|2017-06-21 18:00:...|     CA|      ON|   London| 42.9399| -81.2709|\n4516550|2017-06-21 15:00:...|     CA|      ON|   Guelph| 43.5776| -80.2201|\n4516600|2017-06-21 15:00:...|     CA|      ON|Stratford| 43.3716| -80.9773|\n4516613|2017-06-21 15:00:...|     CA|      ON|Stratford| 43.3716| -80.9773|\n+-------+--------------------+-------+--------+---------+--------+---------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["poi_df_filtered = poi_df.dropDuplicates(['Latitude', 'Longitude'])\n\npoi_df_filtered.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+---------+-----------+\nPOIID| Latitude|  Longitude|\n+-----+---------+-----------+\n POI1|53.546167|-113.485734|\n POI4| 45.22483| -63.232729|\n POI3|45.521629| -73.566024|\n+-----+---------+-----------+\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["data_df_filtered = data_df.groupBy('TimeSt','Latitude','Longitude').count().where('count = 1')\n\ndata_df_filtered.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[62]: 17973</div>"]}}],"execution_count":7},{"cell_type":"code","source":["data_df.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[54]: 22025</div>"]}}],"execution_count":8},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":9}],"metadata":{"name":"pyspark_solution_analysis","notebookId":2604047561355218},"nbformat":4,"nbformat_minor":0}
